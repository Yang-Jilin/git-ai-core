import asyncio
from typing import Dict, Any, List, Optional
import json
import re
from datetime import datetime
from app.core.ai_manager import AIManager
from app.core.mcp_server import McpServer

class IntentRecognizer:
    """æ„å›¾è¯†åˆ«å™¨ - åŸºäºæŸ¥è¯¢å†…å®¹æ™ºèƒ½é€‰æ‹©æ–‡ä»¶"""
    
    FILE_TYPE_PATTERNS = {
        'python': ['.py', 'requirements.txt', 'setup.py', 'pyproject.toml'],
        'javascript': ['.js', '.ts', '.jsx', '.tsx', 'package.json'],
        'config': ['.json', '.yaml', '.yml', '.toml', '.ini', '.env'],
        'documentation': ['.md', '.txt', 'README', 'LICENSE', 'CHANGELOG'],
        'build': ['Dockerfile', 'Makefile', 'docker-compose.yml', '.gitignore']
    }
    
    KEYWORD_MAPPINGS = {
        'åº“': ['requirements.txt', 'package.json', 'pyproject.toml', 'setup.py'],
        'å¯¼å…¥': ['.py', '.js', '.ts'],  # æºä»£ç æ–‡ä»¶
        'ä¾èµ–': ['requirements.txt', 'package.json', 'pyproject.toml'],
        'é…ç½®': ['.env', 'config/', 'settings/', '.json', '.yaml'],
        'æ–‡æ¡£': ['README.md', 'docs/', '.md'],
        'å‡½æ•°': ['.py', '.js', '.ts'],  # æºä»£ç æ–‡ä»¶
        'ç±»': ['.py', '.js', '.ts'],   # æºä»£ç æ–‡ä»¶
        'æ¨¡å—': ['.py', '.js', '.ts'], # æºä»£ç æ–‡ä»¶
        'åŒ…': ['requirements.txt', 'package.json'],
        'å®‰è£…': ['requirements.txt', 'package.json', 'setup.py'],
        'ä»£ç ': ['.py', '.js', '.ts', '.java', '.cpp', '.c'],
        'é¡¹ç›®': ['README.md', 'package.json', 'pyproject.toml'],
        'æ¶æ„': ['README.md', 'docs/', 'architecture.md'],
        'æµ‹è¯•': ['test/', 'tests/', '.test.', '.spec.']
    }
    
    def __init__(self):
        from app.core.project_mcp_server import project_mcp_server
        self.project_mcp_server = project_mcp_server
    
    def extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        cleaned_query = re.sub(r'[^\w\s]', ' ', query)
        # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
        words = cleaned_query.split()
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 'å“ªäº›', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•'}
        keywords = [word for word in words if word not in stop_words and len(word) > 1]
        return keywords
    
    def identify_file_types(self, keywords: List[str]) -> List[str]:
        """æ ¹æ®å…³é”®è¯è¯†åˆ«æ–‡ä»¶ç±»å‹"""
        file_types = []
        for keyword in keywords:
            for file_type, patterns in self.FILE_TYPE_PATTERNS.items():
                if any(pattern in keyword for pattern in patterns):
                    if file_type not in file_types:
                        file_types.append(file_type)
        return file_types if file_types else ['python', 'javascript', 'config']  # é»˜è®¤ç±»å‹
    
    async def get_project_structure(self, project_path: str) -> Dict[str, Any]:
        """è·å–é¡¹ç›®æ–‡ä»¶ç»“æ„"""
        try:
            result = await self.project_mcp_server.list_project_files(project_path, max_depth=10)
            
            if result.get("success") and result.get("files"):
                # å°†æ–‡ä»¶åˆ—è¡¨è½¬æ¢ä¸ºæ ‘å½¢ç»“æ„
                return self._build_file_tree(result["files"])
            else:
                return {"name": "project", "type": "directory", "children": []}
                
        except Exception as e:
            print(f"è·å–é¡¹ç›®ç»“æ„å¤±è´¥: {str(e)}")
            return {"name": "project", "type": "directory", "children": []}
    
    def _build_file_tree(self, files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°†æ–‡ä»¶åˆ—è¡¨æ„å»ºä¸ºæ ‘å½¢ç»“æ„"""
        root = {"name": "project", "type": "directory", "children": []}
        
        for item in files:
            path_parts = item["path"].split('/')
            current_level = root["children"]
            
            for i, part in enumerate(path_parts):
                if i == len(path_parts) - 1:
                    # è¿™æ˜¯æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼Œæ˜¯æ–‡ä»¶
                    current_level.append({
                        "name": part,
                        "type": "file",
                        "path": item["path"]
                    })
                else:
                    # æŸ¥æ‰¾æˆ–åˆ›å»ºç›®å½•
                    found_dir = None
                    for child in current_level:
                        if child["type"] == "directory" and child["name"] == part:
                            found_dir = child
                            break
                    
                    if not found_dir:
                        found_dir = {
                            "name": part,
                            "type": "directory",
                            "children": []
                        }
                        current_level.append(found_dir)
                    
                    current_level = found_dir["children"]
        
        return root
    
    def find_files_in_tree(self, tree: Dict[str, Any], patterns: List[str]) -> List[str]:
        """åœ¨æ–‡ä»¶æ ‘ä¸­æŸ¥æ‰¾åŒ¹é…æ¨¡å¼çš„æ–‡ä»¶"""
        files = []
        
        if tree.get("type") == "file":
            file_name = tree.get("name", "")
            if any(pattern.lower() in file_name.lower() for pattern in patterns):
                files.append(file_name)
        
        elif tree.get("type") == "directory":
            for child in tree.get("children", []):
                files.extend(self.find_files_in_tree(child, patterns))
        
        return files
    
    def match_files_to_query(self, project_structure: Dict[str, Any], keywords: List[str], file_types: List[str]) -> List[Dict[str, Any]]:
        """åŒ¹é…æŸ¥è¯¢åˆ°å…·ä½“çš„æ–‡ä»¶"""
        suggested_files = []
        
        # 1. ç²¾ç¡®æ–‡ä»¶ååŒ¹é…ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        for keyword in keywords:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“çš„æ–‡ä»¶åå…³é”®è¯
            file_name_patterns = ['.py', '.js', '.ts', '.json', '.md', '.txt', '.yaml', '.yml']
            if any(pattern in keyword for pattern in file_name_patterns):
                # å°è¯•ç›´æ¥åŒ¹é…æ–‡ä»¶å
                matched_files = self.find_files_in_tree(project_structure, [keyword])
                for file_path in matched_files:
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": f"ç²¾ç¡®æ–‡ä»¶ååŒ¹é… '{keyword}'",
                        "priority": 20  # æœ€é«˜ä¼˜å…ˆçº§
                    })
        
        # 2. åŸºäºå…³é”®è¯çš„ç›´æ¥åŒ¹é…
        for keyword in keywords:
            if keyword in self.KEYWORD_MAPPINGS:
                patterns = self.KEYWORD_MAPPINGS[keyword]
                matched_files = self.find_files_in_tree(project_structure, patterns)
                for file_path in matched_files:
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡
                    if not any(f["file_path"] == file_path for f in suggested_files):
                        suggested_files.append({
                            "file_path": file_path,
                            "reason": f"å…³é”®è¯ '{keyword}' åŒ¹é…",
                            "priority": 15  # é«˜ä¼˜å…ˆçº§
                        })
        
        # 3. åŸºäºæŸ¥è¯¢æ„å›¾çš„ç‰¹æ®Šå¤„ç†
        query_context = " ".join(keywords).lower()
        
        # å¦‚æœæ˜¯å…³äºç‰¹å®šæ–‡ä»¶çš„æŸ¥è¯¢
        if any(term in query_context for term in ['ai.py', 'aiæ–‡ä»¶', 'aiæ¨¡å—']):
            ai_files = self.find_files_in_tree(project_structure, ['ai.py', 'ai_', '_ai'])
            for file_path in ai_files:
                if not any(f["file_path"] == file_path for f in suggested_files):
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": "AIç›¸å…³æ–‡ä»¶",
                        "priority": 18  # è¾ƒé«˜ä¼˜å…ˆçº§
                    })
        
        # å¦‚æœæ˜¯å…³äºä¾èµ–çš„æŸ¥è¯¢
        if any(term in query_context for term in ['ä¾èµ–', 'åº“', 'package', 'requirement']):
            dep_files = self.find_files_in_tree(project_structure, ['requirements.txt', 'package.json', 'pyproject.toml'])
            for file_path in dep_files:
                if not any(f["file_path"] == file_path for f in suggested_files):
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": "ä¾èµ–é…ç½®æ–‡ä»¶",
                        "priority": 16  # é«˜ä¼˜å…ˆçº§
                    })
        
        # 4. åŸºäºæ–‡ä»¶ç±»å‹çš„åŒ¹é…
        for file_type in file_types:
            patterns = self.FILE_TYPE_PATTERNS[file_type]
            matched_files = self.find_files_in_tree(project_structure, patterns)
            for file_path in matched_files:
                # é¿å…é‡å¤æ·»åŠ 
                if not any(f["file_path"] == file_path for f in suggested_files):
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": f"æ–‡ä»¶ç±»å‹ '{file_type}' åŒ¹é…",
                        "priority": 10  # ä¸­ä¼˜å…ˆçº§
                    })
        
        # 5. ç¡®ä¿åŒ…å«å¸¸è§é…ç½®æ–‡ä»¶ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
        common_configs = ['README.md', 'package.json', 'requirements.txt', 'pyproject.toml', 'setup.py']
        for config_file in common_configs:
            config_files = self.find_files_in_tree(project_structure, [config_file])
            for file_path in config_files:
                if not any(f["file_path"] == file_path for f in suggested_files):
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": "å¸¸è§é¡¹ç›®é…ç½®æ–‡ä»¶",
                        "priority": 5  # ä½ä¼˜å…ˆçº§
                    })
        
        return suggested_files
    
    def prioritize_and_deduplicate(self, files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜å…ˆçº§æ’åºå’Œå»é‡"""
        # å»é‡
        unique_files = {}
        for file_info in files:
            file_path = file_info["file_path"]
            if file_path not in unique_files or file_info["priority"] > unique_files[file_path]["priority"]:
                unique_files[file_path] = file_info
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_files = sorted(unique_files.values(), key=lambda x: x["priority"], reverse=True)
        
        return sorted_files
    
    async def analyze_query(self, project_path: str, query: str) -> List[Dict[str, Any]]:
        """åˆ†ææŸ¥è¯¢å¹¶è¿”å›éœ€è¦è¯»å–çš„æ–‡ä»¶åˆ—è¡¨"""
        print(f"ğŸ” åˆ†ææŸ¥è¯¢: '{query}'")
        
        # 1. å…³é”®è¯æå–å’Œåˆ†ç±»
        keywords = self.extract_keywords(query.lower())
        print(f"  æå–å…³é”®è¯: {keywords}")
        
        file_types = self.identify_file_types(keywords)
        print(f"  è¯†åˆ«æ–‡ä»¶ç±»å‹: {file_types}")
        
        # 2. è·å–é¡¹ç›®ç»“æ„
        project_structure = await self.get_project_structure(project_path)
        
        # 3. æ™ºèƒ½æ–‡ä»¶åŒ¹é…
        suggested_files = self.match_files_to_query(project_structure, keywords, file_types)
        print(f"  åŒ¹é…åˆ° {len(suggested_files)} ä¸ªæ–‡ä»¶")
        
        # 4. ä¼˜å…ˆçº§æ’åºå’Œå»é‡
        prioritized_files = self.prioritize_and_deduplicate(suggested_files)
        print(f"  å»é‡åå‰©ä½™ {len(prioritized_files)} ä¸ªæ–‡ä»¶")
        
        return prioritized_files[:8]  # è¿”å›æœ€å¤š8ä¸ªæ–‡ä»¶


class AutoFileReader:
    """è‡ªåŠ¨æ–‡ä»¶è¯»å–å™¨ - æ— éœ€ç”¨æˆ·æ‰¹å‡†"""
    
    def __init__(self):
        from app.core.project_mcp_server import project_mcp_server
        self.project_mcp_server = project_mcp_server
    
    async def read_files(self, project_path: str, file_requests: List[Dict[str, Any]]) -> Dict[str, str]:
        """è‡ªåŠ¨è¯»å–å¤šä¸ªæ–‡ä»¶"""
        file_contents = {}
        
        print(f"ğŸ“– å¼€å§‹è¯»å– {len(file_requests)} ä¸ªæ–‡ä»¶...")
        
        for file_request in file_requests:
            file_path = file_request["file_path"]
            reason = file_request.get("reason", "")
            
            try:
                content = await self.read_file(project_path, file_path)
                if content:
                    file_contents[file_path] = content
                    print(f"   âœ“ {file_path} - {reason}")
                else:
                    print(f"   âš ï¸ {file_path} - æ–‡ä»¶ä¸ºç©º")
            except Exception as e:
                print(f"   âŒ {file_path} - è¯»å–å¤±è´¥: {str(e)}")
                # å°è¯•è·¯å¾„ä¿®æ­£
                corrected_path = await self.try_correct_path(project_path, file_path)
                if corrected_path:
                    try:
                        content = await self.read_file(project_path, corrected_path)
                        if content:
                            file_contents[corrected_path] = content
                            print(f"   âœ“ {corrected_path} - è·¯å¾„ä¿®æ­£åæˆåŠŸè¯»å–")
                    except:
                        print(f"   âŒ {corrected_path} - è·¯å¾„ä¿®æ­£åä»ç„¶å¤±è´¥")
        
        print(f"âœ… æˆåŠŸè¯»å– {len(file_contents)}/{len(file_requests)} ä¸ªæ–‡ä»¶")
        return file_contents
    
    async def read_file(self, project_path: str, file_path: str) -> Optional[str]:
        """è¯»å–å•ä¸ªæ–‡ä»¶"""
        try:
            result = await self.project_mcp_server.read_project_file(project_path, file_path)
            
            if result.get("success") and result.get("content"):
                return result["content"]
            return None
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¼‚å¸¸ {file_path}: {str(e)}")
            return None
    
    async def try_correct_path(self, project_path: str, original_path: str) -> Optional[str]:
        """å°è¯•ä¿®æ­£æ–‡ä»¶è·¯å¾„"""
        # ç®€å•çš„è·¯å¾„ä¿®æ­£é€»è¾‘
        corrections = [
            original_path,
            original_path.lower(),
            original_path.upper(),
            "./" + original_path,
            original_path.lstrip('./'),
        ]
        
        # å¯¹äºå¸¸è§çš„é…ç½®æ–‡ä»¶ï¼Œå°è¯•æ ‡å‡†ä½ç½®
        common_files = {
            'requirements.txt': ['requirements.txt', 'reqs.txt'],
            'package.json': ['package.json', 'package-lock.json'],
            'README.md': ['README.md', 'readme.md', 'Readme.md'],
            'pyproject.toml': ['pyproject.toml'],
            'setup.py': ['setup.py']
        }
        
        if original_path in common_files:
            corrections.extend(common_files[original_path])
        
        # æµ‹è¯•æ¯ä¸ªä¿®æ­£åçš„è·¯å¾„
        for corrected_path in corrections:
            if corrected_path == original_path:
                continue
                
            try:
                result = await self.project_mcp_server.read_project_file(project_path, corrected_path)
                if result.get("success") and result.get("content"):
                    return corrected_path
            except:
                continue
        
        return None


class FileContextTracker:
    """æ–‡ä»¶ä¸Šä¸‹æ–‡è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.read_history = {}  # æ–‡ä»¶è¯»å–å†å²
        self.project_context = {}  # é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    def track_file_read(self, file_path: str, content: str):
        """è®°å½•æ–‡ä»¶è¯»å–å†å²"""
        preview = content[:200] + "..." if len(content) > 200 else content
        self.read_history[file_path] = {
            "timestamp": datetime.now().isoformat(),
            "preview": preview,
            "content_length": len(content)
        }
    
    def get_relevant_context(self, current_query: str) -> Dict[str, Any]:
        """è·å–ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        relevant_files = {}
        keywords = current_query.lower().split()
        
        for file_path, info in self.read_history.items():
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            file_match = any(keyword in file_path.lower() for keyword in keywords)
            content_match = any(keyword in info["preview"].lower() for keyword in keywords)
            
            if file_match or content_match:
                relevant_files[file_path] = info
        
        return relevant_files
    
    def get_read_history(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´çš„è¯»å–å†å²"""
        return self.read_history


class AdvancedSmartConversationManager:
    """é«˜çº§æ™ºèƒ½å¯¹è¯ç®¡ç†å™¨ - ç±»Clineæ¶æ„"""
    
    def __init__(self):
        self.ai_manager = AIManager()
        self.intent_recognizer = IntentRecognizer()
        self.file_reader = AutoFileReader()
        self.file_context_tracker = FileContextTracker()
        self.conversations = {}
    
    def _get_ai_config(self) -> Dict[str, Any]:
        """è·å–AIé…ç½® - æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°è¯»å–é…ç½®æ–‡ä»¶"""
        try:
            import json
            import os
            
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_dir = os.path.dirname(__file__)
            
            # æ„å»ºæ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä»å½“å‰æ–‡ä»¶ä½ç½®è®¡ç®—ï¼‰
            config_paths = [
                os.path.join(current_dir, '..', 'api', 'AI-Config.json'),  # backend/app/api/AI-Config.json
                os.path.join(current_dir, '..', '..', 'api', 'AI-Config.json'),  # backend/api/AI-Config.json
                os.path.join(current_dir, '..', '..', '..', 'AI-Config.json'),   # AI-Config.json
            ]
            
            # æ·»åŠ ç»å¯¹è·¯å¾„æ£€æŸ¥å¹¶å»é‡
            abs_config_paths = []
            for path in config_paths:
                abs_path = os.path.abspath(path)
                if abs_path not in abs_config_paths:
                    abs_config_paths.append(abs_path)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” æŸ¥æ‰¾AIé…ç½®æ–‡ä»¶ï¼Œæ£€æŸ¥ä»¥ä¸‹è·¯å¾„:")
            for i, path in enumerate(abs_config_paths, 1):
                exists = os.path.exists(path)
                print(f"  {i}. {path} - {'âœ… å­˜åœ¨' if exists else 'âŒ ä¸å­˜åœ¨'}")
            
            config = None
            config_path = None
            for path in abs_config_paths:
                if os.path.exists(path):
                    config_path = path
                    print(f"ğŸ“„ è¯»å–AIé…ç½®æ–‡ä»¶: {path}")
                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                        print(f"âœ… æˆåŠŸè¯»å–é…ç½®æ–‡ä»¶")
                        break
                    except Exception as e:
                        print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {path}: {str(e)}")
                        continue
            
            if config:
                print(f"ğŸ¤– ä½¿ç”¨AIé…ç½®: {config.get('ai_provider', 'unknown')}")
                # éªŒè¯å¿…è¦çš„é…ç½®å­—æ®µ
                api_key = config.get("ai_api_key", "")
                if not api_key:
                    print("âš ï¸ è­¦å‘Š: AIé…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘api_key")
                
                return {
                    "provider": config.get("ai_provider", "openai"),
                    "model": config.get("ai_model", "gpt-4o-mini"),
                    "api_key": api_key,
                    "base_url": config.get("ai_base_url")
                }
            
            # æ£€æŸ¥ç¯å¢ƒå˜é‡
            print("â„¹ï¸ æœªæ‰¾åˆ°AIé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡")
            env_api_key = os.getenv("AI_API_KEY", "")
            if not env_api_key:
                print("âš ï¸ è­¦å‘Š: ç¯å¢ƒå˜é‡ä¸­ç¼ºå°‘AI_API_KEY")
            
            return {
                "provider": os.getenv("AI_PROVIDER", "openai"),
                "model": os.getenv("AI_MODEL", "gpt-4o-mini"),
                "api_key": env_api_key,
                "base_url": os.getenv("AI_BASE_URL")
            }
            
        except Exception as e:
            print(f"âŒ è¯»å–AIé…ç½®å¤±è´¥: {str(e)}")
            return {
                "provider": "openai",
                "model": "gpt-4o-mini",
                "api_key": "",
                "base_url": None
            }
    
    async def generate_response(self, project_path: str, user_query: str, file_contents: Dict[str, str], context: Dict[str, Any] = None) -> str:
        """åŸºäºæ–‡ä»¶å†…å®¹ç”Ÿæˆå›ç­”"""
        # æ„å»ºæ–‡ä»¶å†…å®¹æ‘˜è¦
        file_summary = "\n".join([
            f"æ–‡ä»¶: {file_path}\nå†…å®¹:\n{content[:1000]}...\n{'-'*50}"
            for file_path, content in file_contents.items()
        ])
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = ""
        if context:
            context_info = f"\nç›¸å…³ä¸Šä¸‹æ–‡æ–‡ä»¶:\n" + "\n".join([
                f"- {file_path} (ä¸Šæ¬¡è¯»å–: {info['timestamp']})"
                for file_path, info in context.items()
            ]) + "\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡ä»¶å†…å®¹ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {user_query}
é¡¹ç›®è·¯å¾„: {project_path}
{context_info}
ç›¸å…³æ–‡ä»¶å†…å®¹:
{file_summary}

è¯·ç›´æ¥é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®çš„ç­”æ¡ˆï¼Œä¿æŒå›ç­”ç®€æ´ã€ç›´æ¥ã€å®ç”¨ã€‚
è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è‡ªç„¶æ˜“æ‡‚ã€‚"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºä»£ç æ–‡ä»¶å†…å®¹æä¾›æ·±å…¥çš„é¡¹ç›®åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        ai_config = self._get_ai_config()
        
        response = await self.ai_manager.chat(
            provider=ai_config["provider"],
            model=ai_config["model"],
            messages=messages,
            api_key=ai_config["api_key"],
            base_url=ai_config.get("base_url"),
            temperature=0.7,
            max_tokens=1500
        )
        
        return response["content"]
    
    async def process_smart_chat(self, conversation_id: str, project_path: str, user_query: str) -> Dict[str, Any]:
        """å¤„ç†æ™ºèƒ½å¯¹è¯è¯·æ±‚"""
        print(f"\nğŸ¤– === é«˜çº§æ™ºèƒ½åˆ†æå¼€å§‹ ===")
        print(f"ä¼šè¯ID: {conversation_id}")
        print(f"é¡¹ç›®è·¯å¾„: {project_path}")
        print(f"ç”¨æˆ·æŸ¥è¯¢: '{user_query}'")
        print("=" * 50)
        
        try:
            # 1. æ„å›¾åˆ†æå’Œæ–‡ä»¶é€‰æ‹©
            print("ğŸ” é˜¶æ®µ1: åˆ†ææŸ¥è¯¢æ„å›¾å’Œé€‰æ‹©æ–‡ä»¶...")
            file_requests = await self.intent_recognizer.analyze_query(project_path, user_query)
            
            if not file_requests:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶åˆ—è¡¨")
                file_requests = [
                    {"file_path": "README.md", "reason": "é»˜è®¤é¡¹ç›®æ–‡æ¡£", "priority": 1},
                    {"file_path": "package.json", "reason": "é»˜è®¤é¡¹ç›®é…ç½®", "priority": 1}
                ]
            
            print(f"ğŸ“ é€‰æ‹©çš„æ–‡ä»¶:")
            for i, req in enumerate(file_requests, 1):
                print(f"  {i}. {req['file_path']} - {req.get('reason', 'æ— åŸå› ')}")
            
            # 2. è‡ªåŠ¨æ–‡ä»¶è¯»å–
            print("\nğŸ“– é˜¶æ®µ2: è‡ªåŠ¨è¯»å–æ–‡ä»¶...")
            file_contents = await self.file_reader.read_files(project_path, file_requests)
            
            # 3. æ›´æ–°ä¸Šä¸‹æ–‡
            for file_path, content in file_contents.items():
                self.file_context_tracker.track_file_read(file_path, content)
            
            # 4. è·å–ç›¸å…³ä¸Šä¸‹æ–‡
            context = self.file_context_tracker.get_relevant_context(user_query)
            
            # 5. æ™ºèƒ½å›ç­”ç”Ÿæˆ
            print("\nğŸ’¡ é˜¶æ®µ3: ç”Ÿæˆæ™ºèƒ½å›ç­”...")
            response_content = await self.generate_response(project_path, user_query, file_contents, context)
            
            # 6. è¿”å›ç»“æœ
            print("âœ… åˆ†æå®Œæˆ")
            
            # æ„å»ºå·¥å…·è°ƒç”¨ç»“æœ
            tool_calls = []
            for req in file_requests:
                file_path = req["file_path"]
                tool_calls.append({
                    "tool_name": "read_project_file",
                    "arguments": {"project_path": project_path, "file_path": file_path},
                    "result": {
                        "success": file_path in file_contents,
                        "content": file_contents.get(file_path, ""),
                        "file_path": file_path
                    },
                    "reason": req.get("reason", "")
                })
            
            return {
                "response": response_content,
                "tool_calls": tool_calls,
                "conversation_id": conversation_id,
                "analysis_context": {
                    "query": user_query,
                    "selected_files": file_requests,
                    "successful_reads": len(file_contents),
                    "context_files": list(context.keys())
                }
            }
            
        except Exception as e:
            error_msg = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(f"âŒ é”™è¯¯: {error_msg}")
            print("=" * 50)
            return {
                "response": error_msg,
                "tool_calls": [],
                "conversation_id": conversation_id,
                "error": str(e)
            }


# åˆ›å»ºå…¨å±€å®ä¾‹
advanced_smart_conversation_manager = AdvancedSmartConversationManager()
